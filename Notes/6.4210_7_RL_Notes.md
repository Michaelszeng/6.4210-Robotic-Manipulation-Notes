# Reinforcement Learning

***Why?***

Previous approaches discussed have a 2 part pipeline:
1) estimate the state of the world
2) decide what to do based on the state of the world

This pipeline is called *"Model-based" Control*.

This approach is probably unideal; imagine:
 - chopping/dicing onions into hundreds of tiny pieces
 - playing with ropes
 - spreading peanut butter

These scenarios are all incredibly hard to represent in terms of a state and difficult to estimate the state of.

Instead: learn *visuomotor policy*--mapping camera/sensors directly to $q$, where $q$ contains both the state of the robot (joint positions) and state of elements in the world.

## The Simplest Solution: Behavior Cloning/Imitation Learning

Have a human (with VR, haptics) control the robot. Have an RNN learn from the behaviors of the human.

THIS IS STILL A SUPERVISED LEARNING PROBLEM. Learning the parameters of a NN that maps camera inputs to iiwa commands fmom training data.

Limitations:
 - labor (humans have to generate the data)
 - limited by capabilities of human controllers

Behavior cloning performs very well within the convex hull of the demonstration data; i.e. it's good at interpolating actions within the action space of the demonstrations, very bad at extrapolating actions for new scenarios.

Therefore, behavior cloning also relies on a notion of stability; the robot should naturally stay within the convex hull of the demonstration data.

You can also write an optimal controller using non-RL methods, run them continously in simulation, and have the behavior cloner learn from the simulation.


### A Full Behavior Cloning Pipeline:

Feed RGB images to a large pre-trained image recognition model, remove it's final layer (that outputs an image classification), take the output of the 2nd to last layer, mix it with some other robot sensors, feed as input to a small learned network that outputs robot actions directly.

In this sense, the pre-trained instance segmentation model is doing the bulk of the work; the control problem is just sort of "fine-tuning" the instance segmentation model.

<center><img src="simple_rl.png" alt="simple_rl" style="width:70%" /></center><br />


## Diffusion

Traditional diffusion for images: Take image dataset, add random Gaussian noise, then train model to go backwards from noisy image to clean image

This can be applied to robotics; i.e. for a given trajectory, you can learn to denoise any point in 3D space to get back onto a desired trajectory

Won't get into the technical details, but diffusion models & transformer models are performing extremely well on small amounts of demonstration data.

The general direction of the industry lies on a bet that Russ and others are making: if you do enough examples on arbitrary tasks, eventually, the robot can learn some kind of core principle of doing things in general, and the robot will be able to do new tasks as well.


# RL Math

$$ u[n] = \Pi_\alpha(y[n], u[n-1], y[n-1], ~ ...) $$

where $u$ are the outputted controls/actions of the policy, and $y$ is the state. Need to learn/optimize parameters $\alpha$ of policy $\Pi$.

(Scalar, one-step) Reward function:

$$ r(y[n], u[n]) $$

Therefore, the goal is:

$$ \max\limits_{\alpha} \sum_n r(y[n], u[n]) $$
$$ \text{Controller: } ~~ x[n+1] = f(x[n], u[n]) ~~~~ x[0] = x_0 $$
$$ y[n] = g(x[n], u[n]) $$
$$ \text{Policy: } ~~ u[n] = \Pi_{\alpha}(y[n], u[n-1], y[n-1], ~ ...) $$

RL is just one approach to solve this optimal control problem, emphasizes stochastic trial/error.

We re-express the optimization with randomness:

$$ \max\limits_{\alpha} E [\sum_n r(y[n], u[n])] $$
$$ \text{Controller: } ~~ P_x(x[n+1] | x[n], u[n]) $$
$$ ~~~~~~~~~~~~~~ P_y(x[n] | x[n], u[n]) $$
$$ \text{Policy: } ~~ u[n] = \Pi_{\alpha}(y[n], u[n-1], y[n-1], ~ ...) $$

where $E$ is the expected value, and $P$ are all probabilistic functions.

## PPO (Proximal Policy Opt.)

**An Aside: Weight Perturbation**

Eval $f(\alpha)$ and $f(\alpha + w)$ where $w$ is some small disturbance.

Perform a modified "gradient descent": $x = x - \eta[f(\alpha + w) - f(\alpha)]w$

Intuitively, if $f(\alpha + w) > f(\alpha)$, then the perterbation $w$ is in the opposite direction as the downward gradient, then $w$ multiplied by a negative constant, so we still progress down the gradient. If, instead, $f(\alpha) > f(\alpha + w)$, then the perterbation $w$ is in the direction of the gradient, so $w$ gets multiplied by a positive constant, so we still progress down the gradient.

In technical terms, the expected value $[f(\alpha + w) - f(\alpha)]$ is proportional to the analytic gradient $\frac{\delta f}{\delta \alpha}$.

<center><img src="weight_perturb.png" alt="weight_perturb" style="width:40%" /></center><br />

**Back to PPO**: PPO is a policy gradient RL algorithm.

### Policy Gradient (REINFORCE Algorithm)

Say our goal is to minimize the cost of our policy:

$$ \min\limits_{\alpha} E[f(x)] $$

where $x$ is selected from a probability distribution depending on parameters (to be learned) $\alpha$: $x \sim p_{\alpha}(x)$.

We can optimize this using gradient descent, where the gradient of $E[f(x)]$ is $\frac{\delta}{\delta x} E[f(x)] = E[f(x) \frac{delta}{\delta \alpha} ln (p_{\alpha}(x))]$. This is called the log-likelihood method of taking the gradient of an expected value. From a practical standpoint, to calculate the gradient, we would evaluate $f(x) \frac{delta}{\delta \alpha} ln (p_{\alpha}(x))$ for a number of samples (each one with a different random vlaue of $x$) and average them to get the expected value.

To understand this better, we go through an example of finding the gradient of $E[f(x)]$. Say $x \sim p_{\alpha}(x) \sim N(\alpha, \sigma^2) \sim \alpha + N(0, \sigma^2)$ (where $N(a, b)$ is normal/Gaussian distribution around $a$ with STD $b$). Call $\beta \sim N(0, \sigma^2)$, a randomly sampled value; notice that $\beta = \alpha-x$.
1) Expanding the Gaussian, we get $p_{\alpha}(x) = c*e^{\frac{-(x-\alpha)^T(x-\alpha)}{2 \sigma^2}}$ for some constant c. 
2) Taking $ln$ of $p_{\alpha}(x)$, we get: $\frac{-(x-\alpha)^T(x-\alpha)}{2 \sigma^2} + ln(c)$.
3) Taking $\frac{delta}{\delta \alpha}$ of $ln(p_{\alpha}(x))$, we get: $\frac{(\alpha-x)^T}{\sigma^2} = \frac{1}{\sigma^2}\beta^T$.
4) Finally, $f(x) \frac{delta}{\delta \alpha} ln (p_{\alpha}(x)) = \frac{1}{\sigma^2} f(\alpha+\beta)\beta^T$
5) The gradient update there is $\Delta \alpha = - \eta*E[\frac{1}{\sigma^2} f(\alpha+\beta)\beta^T]$

Notice: $E[\frac{1}{\sigma^2} f(\alpha+\beta)\beta^T] = \frac{1}{\sigma^2}*E[(\alpha+\beta) - f(\alpha)]\beta^T$; this is true because $f(\alpha)$ has a constant value --> $E[f(\alpha)\beta^T] = E[f(\alpha)]*E[\beta^T] = 0$ bc $E[\beta^T]=0$ (given that $\beta \sim n(0, \sigma^2)$, as we defined above.)

Now, the $f(\alpha+\beta) - f(\alpha)$ looks identical to the weight perturbation equation above. Therefore, we don't actually have to sample many values and take the expected value; we can just set the gradient update to $\Delta \alpha = - \frac{1}{\sigma^2} *(f(\alpha+\beta) - f(\alpha))\beta^T$ and get a "gradient" that is on average in the right direction. This is good because this "weight perturbation" version of the gradient is much faster to compute than the analytical gradient (which requires sampling many $\beta$ to calculate the expected value at each step).

#### What Happens to Non-continuous Cost Landscapes?

The plot of $f(x)$ vs $x$ is non-continuous (i.e. the cost has jumps).

You might think gradient descent would fail, but actually, expected value does a "randomized smoothing" that makes gradent descent still work. Because, for every $x$, $p_{\alpha}(x)$ may be slightly offset, there will be some cases where $x$ to the left of a continuity gets sampled as an $x$ to the right, and vice versa. Therefore, $E[f(x)]$ doesn't equal exactly $f(x)$; it gets influenced by the values of $f(x)$ on the otherside of the discontinuity, resulting in a smoothing effect of the optimization landscape. The plot of $E[f(x)]$ vs $x$ is continuous.

"Non-smooth Optimization" - the idea that gradient descent always works better when $f(x)$ is taken on a randomly sampled $x$ (from i.e. a Normal distribution), because gradient descent works best on smoother optimization landscapes.


### Software Tools for RL

Gymnasium (formerly OpenAI Gym)

Stable Baselines 3 (collection of implementations for RL algorithms)
 - Most common for sim: PPO (Proximal Policy Opt.)
 - Surprisingly, most implementations of RL just use these implementations (harder to create one from scratch and make it better)


Most problems can be solved with RL (with lots of compute power).
 - Importance of Simulation and reward function tuning: with simulation, you can do tons of training. Also, RL often learns illogical things; you need a comprehensive reward function to encode common-sensical constraints. May also need reward-shaping for complex problems, where reward doesn't obviously map to a single action.

For sim2real, common implementations first train in an inaccurate but high-quantity (can train 10,000 robots at once on a GPU) sim to go from "nothing" to "something", then do a second round of real-life training.